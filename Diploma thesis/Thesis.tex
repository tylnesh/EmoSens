\documentclass[11pt,a4paper]{report}
\usepackage[cp1250]{inputenc}
\usepackage[english]{babel}
\usepackage[IL2]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{subfig}
\usepackage[autostyle]{csquotes}
\usepackage{setspace}
\usepackage[
backend=bibtex        % if we want unicode
,style=iso-authoryear % or iso-numeric for numeric citation method
,autolang=other       % to support multiple languages in bibliography      
]{biblatex}
\usepackage{multicol}
\usepackage{titlesec}    
\titleformat{\chapter}[block]
{\normalfont\Large\bfseries}{\thechapter.}{1em}{\Large}
\titlespacing*{\chapter}{0pt}{-19pt}{19pt}


\titleformat{\section}[block]
{\normalfont\large\bfseries}{\thesection.}{1em}{\large}
\titlespacing*{\section}{0pt}{11pt}{19pt}


\usepackage{scrpage2}
\ifoot[]{}
\cfoot[]{}
\ofoot[\pagemark]{\pagemark}


\newcommand{\fakeparagraph}[1]{%
	\par\refstepcounter{paragraph}% Increase paragraph counter
	\paragraphmark{#1}% Add paragraph mark (header)
}



\graphicspath{ {./images/} }

\makeatletter
\def\blx@maxline{77}
\makeatother

\bibliography{Thesis.bib}

\newcommand*{\captionsource}[2]{%
	\caption[{#1}]{%
		#1%
		\\\hspace{\linewidth}%
		\textbf{Source:} #2%
	}%
}

\pagenumbering{gobble}

\usepackage[left=3.5cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\author{Bc. Michal Koh˙tek}
\title{Rozpozn·vanie emocion·lneho stavu pouûÌvateæa pomocou inteligentn˝ch 
riadiacich systÈmov}
\begin{document}
	
\begin{titlepage}
	
{\centering
{\bfseries\LARGE UNIVERZITA KONäTANTÕNA FILOZOFA V NITRE \par}
{\bfseries\LARGE FAKULTA PRÕRODN›CH VIED \par}
\vfill
{\bfseries\LARGE ROZPOZN¡VANIE EMOCION¡LNEHO STAVU POUéÕVATEºA POMOCOU 
INTELIGENTN›CH RIADIACICH SYST…MOV\par}
\vspace{1cm}
{\bfseries\Large Diplomov· pr·ca\par}
\par}
\vfill
\Large{
\noindentätudijn˝ program: \quad \thinspace \thinspace 9.2.9 Aplikovan· 
informatika 
\hfill\newline
ätudijn˝ odbor: \qquad \thinspace \thinspace Aplikovan· 
informatika\hfill\newline
äkoliace pracovisko:\enspace\enspace Katedra Informatiky\hfill\newline
äkoliteæ:\qquad\qquad\qquad \thinspace PaedDr. Martin Magdin, PhD.\hfill\par}

\vfill
\noindent\textbf{Nitra 2018}\hfill\textbf{Bc. Michal Koh˙tek}
\end{titlepage}
\onehalfspacing

\null\newpage

\newpage

\vspace*{\fill}
\noindent{\Large \textbf{»estnÈ vyhl·senie}}
\vspace{1cm}\\
Vyhlasujem, ûe som predloûen˙ diplomov˙ pr·cu vypracoval samostatne a ûe 
	som pri jej	tvorbe pouûil literat˙ru, ktor˙ uv·dzam v zozname pouûitej 
	literat˙ry.
\vspace{3cm}\\
V Nitre 20.4.2018 \hfill Podpis...................
\newpage

\vspace*{\fill}
\noindent{\Large \textbf{PoÔakovanie}}
\vspace{1cm}\\
Touto cestou by som sa chcel poÔakovaù hlavne mÙjmu ökoliteæovi, p·novi PaedDr. 
Martinovi Magdinovi, PhD., ktor˝ mi t˙to tÈmu diplomovej pr·ce navrhol a vûdy 
mi s v veækou ochotou pom·hal pri jej tvorbe. œakujem mu za odborn˙ pomoc, 
cennÈ n·pady a usmernenia, ktorÈ mi veæmi pomohli pri pÌsanÌ tejto pr·ce.
\vspace{2cm}\\

\chapter*{Abstrakt}
KOH⁄TEK, Michal: Rozpozn·vanie emocion·lneho stavu pouûÌvateæa za pomoci inteligentn˝ch riadiacich systÈmov. [Diplomov· pr·ca]. Univerzita KonötantÌna Filozofa v Nitre.
Fakulta prÌrodn˝ch vied. äkoliteæ: PaedDr. Martin Magdin, PhD. StupeÚ odbornej
kvalifik·cie: Magister. Nitra: FPV, 2018. 59 s.
\\
\\
V diplomovej pr·ci sa venujeme metodike rozpozn·vania emocion·lneho stavu pouûÌvateæa. Popisujeme a porovn·vame viacerÈ teÛrie a ich implement·ciÌ v komerËn˝ch rieöeniach. V praktickej Ëasti sme vytvorili program s grafick˝m rozhranÌm, ktor˝ kombinuje viacero zdrojov ˙dajov o pouûÌvateæovi a vizualizujem jeho emoËn˝ stav na grafe. Ten sme vysk˙öali na vzorke 12 æudÌ a v˝sledky 8 z nich sme detailne popÌsali.
\\
\\
Kæ˙ËovÈ slov·: EmÛcie, fyziologickÈ prejavy, Affectiva, valencia, Arduino

\chapter*{Abstract}
KOH⁄TEK, Michal: Recognizing user's emotional states using intelligent control systems. [Diploma thesis]. University of Constantine the Philosopher in Nitra. Faculty of
natural sciences. Supervisor: PaedDr. Martin Magdin, PhD. Level of professional
qualifications: Magister. Nitra: FPV, 2018. 59 s.
\\
\\
This diploma thesis is dedicated to the methods of recognizing user's emotional state. We describe and compare various historical thesis and their implementation in commercial solutions. In the practical part we have created a GUI application combining various data sources and visualizes user's emotional state on a chart. This application has been tested on a sample of 12 people, of which we describe 8 in detail.
\\
\\
Keywords: Emotions, Physiological markers, Affectiva, Valence, Arduino


\tableofcontents
 
\chapter*{Introduction}
\pagenumbering{arabic}
\setcounter{page}{8}
\pagestyle{scrplain}
\begin{flushright}

"Smiles are probably the most underrated \\ facial expressions, 
 much more complicated \\ than most people realize. 
 There are dozens\\ of smiles, each differing in appearance \\
 and in the message expressed."\\
 - Paul Ekman
\end{flushright}

\paragraph{} Emotions are at the core of the human experience, albeit very hard 
to define, recognize and name, even in yourself. They are, by definition 
different from person to person, in diverse cultures and upbringings. Our 
perception of emotions and their classification has evolved in recent years. 
Various authors has tried to divide our emotional states into basic categories 
such as Ekman's Anger, Fear, Disgust, Happiness, Sadness and Surprise. However, 
recent work by psychologists and historians alike show, that a more complex 
look at emotions might be needed. 

\paragraph{}Emotional recognition changes with time. In 12th century, bards 
looked at yawning not as a sign of boredom or tiredness, but as a sign of a 
hidden and deep love. Early Christians recognized an emotion called "accidie", 
a lethargy and despair brought about by flying demons. Boredom, as such was 
first really  felt by the Victorians as a response to the new ideas of leisure 
time and self-improvement. Among the psychologist, there is a standing question 
whether some cultures feel some hard to define emotions more strongly, because 
they bothered to name them as separate kinds. For example the Russian "toska", 
a longing with nothing to long for, as coined by Vladimir Nabokov. Recent 
developments of cognitive science tell us, that emotions are not just simple 
reflexes, but inherently complex and elastic systems of response towards both 
the biologies 
that we've inherited and the cultures, that we live in now. They are not just 
simple chemistry, but a cognitive phenomena, not shaped only by our body 
functions, but also our thought process, concepts and language. The 
neuroscientist Lisa Feldman Barrett studies this 
dynamic relationship between words and emotions. She argues, that when a person 
learns a new word for an emotion, they also learn to feel and recognize it 
(\cite{barrett}). 
There is a historicity to emotions, they have changed in history, often times 
very dramatically, in response to new cultural expectations, religious beliefs, 
new ideas about gender, age, ethnicity, economical and political ideologies. 

\paragraph{}There is a push to increase our emotional intelligence. Emotions 
are so 
powerful, that in past, they were sometimes thought to be a cause of illness. 
In 17th century, there was a student attending the Swiss university in Basel. 
He 
came afflicted with fever, heart palpitations, skin sores on his body and was 
close to dying. When they sent him back home to die, he started getting better 
and by the time, he returned to his hometown, he almost entirely recovered. In 
1688, Johannes Hoffer, medical doctor, learnt of this case and 
many like it and coined the term for a severe homesickness as "nostalgia". Last 
confirmed death by nostalgia was an American soldier fighting during the First 
World War in France. In early 20th century, this feeling has morphed more into 
a longing for lost time, instead of homesickness and downgraded in severity. 
Nowadays, our culture celebrates happiness, as it is said to make 
us a better workers, parents and partners. However, in 16th century, this 
position was filled by sadness, as is evident by self-help books from that 
period, which tried to encourage sadness in readers by giving them lists of 
reasons to be disappointed.

\paragraph{}In order to study both basic emotions and their mixtures in the 
complex variety, we need appropriate detection and classification tools and 
techniques. Over the years, many psychologists and computer scientist 
collaborated to create a set of markers and techniques recognizing them. We 
will discuss several of them and compare their usefulness, reliability and 
practicality. These methods most often try to detect basic emotions such as the 
six basic emotions (\cite{Ekman}) or a 12-point Affect Circumplex model 
(\cite{Russel}).
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.6]{images/Primary_Emotions}
		
	\caption{Six primary emotions}

	\label{fig:}
	
\end{figure} 
%\footnote{https://upload.wikimedia.org/wikipedia/commons/3/34/Primary\_Emotions.jpg}

\paragraph{}Affective computing has great many possible applications. It can 
allow for a better education process, providing a large amount of valid 
feedback to the tutors and students (\cite{Asteriadis2008EstimationOB}). It can 
help broaden the knowledge in the study of psychology and psychiatry, improve 
the efficiency of counseling. Since emotions recognition and empathy is one of 
the traits of inter-personal communication, properly analyzing emotions can 
help in developing communicative technologies for use by people with autism. 
Growing interest in affective computing is shown by many industries. Automobile 
manufacturers are partnering with companies like Affectiva and integrating 
their solutions in their high-end cars (\cite{affectiva}). Other venue for 
emotion recognition is targeted marketing. In past, online e-commerce sites 
would often use "heat maps" to see, whether the potential customer can find 
products on the website, and whether the paid advertisement holds their 
attention. If the same system added emotion recognition, online marketers would 
gain greater insight into minds of customers. Another future purpose for 
affective computing may be found in robotic pets and social robots, especially 
those working in health 
care industry. Emotional awareness could allow robotic nurses to better judge 
users' and patient's emotional states and needs (\cite{yonck2017heart}). It is 
clear, that affective computing will grow to become an important aspect of 
computer science theory.




\chapter{Emotion classification} \paragraph{}Emotion classification is a 
contested issue in 
emotion research and in affective science. The two fundamental viewpoints of 
affective scientists' approach are: \begin{enumerate}
	\item Emotions as discrete and fundamentally different constructs
	\item Emotions as fluid and characterized on a dimensional basis in 
	groupings
\end{enumerate}
Various categorizations of emotions also vary in description how emotions 
relate to each other.
\section{Discrete models of emotions}
\paragraph{}Discrete emotion theory claims that there is a small 
number 
of core affects. 
This number can vary depending on the proponent, for example Silvan Tomkins 
considered nine basic emotions. Six, that came evolutionarily earlier, 
interest-excitement, enjoyment-joy, surprise-startle, distress-anguish, 
anger-rage, fear-terror, once that evolved later, shame-humiliation and finally 
disgust and dissmell, which he later took back. In the paired affects, the 
first of the pair is the mild manifestation and the second the more intense. 
(\cite{tomkins1962affect}), (\cite{tomkins1963affect}). This model is somewhat 
controversial nowadays among affective theorists, especially over Tomkins' firm 
insistence that there were nine and only nine, biologically based affects. He 
also argued, that these affects are quite discreet (in contrast to the more 
muddled and complex emotions) and that they shared a common biological heritage 
with what Darwin called emotions in animals (\cite{darwin1998expression}). They 
also differ from Freudian drives in lacking an object.
\paragraph{}Similarly, Carroll Izard delineated 12 discrete emotions: 
interest, joy, surprise, sadness, anger, disgust, contempt, self-hostility, 
fear, shame, shyness and guilt. He measured these via his Differential Emotion 
Scale (\cite{Izard}). Among other contributors to this theory, such as John 
Watson, Edwin Newman and Ross Buck, Paul Ekman performed a series of 
cross-cultural studies with Carrol Izard and reported that there are at least 
six emotions, that people across the world produce and are able to recognize. 
This was further evidenced when researchers approached the people of New Guinea 
with no previous exposure to Westerners nor their culture. When they showed 
them pictures of people expressing six core emotions, subjects could in fact 
point out the different emotions and distinguish between them 
(\cite{ekmanguinea}). 

\section{Dimensional models of emotions}
\paragraph{} There are various theoretical and practical reasons for which some 
researchers define emotions according to one or more dimensions. Dimensional 
models are an attempt to conceptualize human emotions by defining where they 
lie in two or three dimensions. Most incorporate valence and arousal or 
intensity dimensions. In contrast to theories of basic emotion, which propose 
that different emotions arise from separate neural systems, dimensional models 
suggest that a common and interconnected neurophysiological system is 
responsible for all affective states. (\cite{posner}) 
\paragraph{}In 1897, Wilhelm Max Wundt proposed, that emotions can be described 
by three dimensions: 
"pleasurable versus unpleasurable", "arousing versus subduing" and "strain 
versus relaxation" (\cite{wundt2017outlines}). Later, Harold Schlosberg named 
three dimension, "pleasantnessñunpleasantness", "attentionñrejection" and 
"level of activation" (\cite{schlosberg1954three}).
\paragraph{}Another model, called the circumplex model, was developed by James 
Russell. This model suggest that emotions are distributed in a two-dimensional 
circular space, containing both arousal and valence dimensions. The vertical 
axis represents arousal, horizontal axis valence and the center of the circle 
represents a neutral valence and medium level of arousal.(\cite{circumplex}).

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.4]{images/russel_model}
	\caption{ A. Russellís (1980) circumplex model of affect}
	\label{fig:}
\end{figure}
%\footnote{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2367156/bin/nihms44490f1.jpg}
This model was later modified by Russel and Lisa Feldman Barret, which they 
described as representative of core affect, which are the most elementary 
feelings that need not be directed toward anything. Different prototypical 
emotional episodes, or clear emotions that are evoked or directed by specific 
objects, can be plotted on the circumplex, according to their levels of arousal 
and pleasure (\cite{Russell1999CoreAP}).

\paragraph{}Another model of emotion appeared in 1992. This two-dimensional 
model consists of vectors, that point in two directions. The vector model 
assumes that there is always an underlying arousal dimension and that valence 
determines the direction in which a particular emotion lies.
\begin{figure}
		\centering
		\subfloat[2D
		representation]{\includegraphics[width=0.2\linewidth]{images/vector0}}%
		\qquad
		\subfloat[3D 
		representation]{\includegraphics[width=0.2\linewidth]{images/vector1}}%
		\caption{Vector model of emotions}
		\label{fig:}
\end{figure}

\paragraph{} The positive activation ñ negative activation (PANA) was 
originally created in 1985 by David Watson and Auke Tellegen. It suggests that 
positive and negative affect are two separate systems. Like in the vector 
model, states of higher arousal tend to be defined by their valence and states 
of lower arousal tend to be more neutral in term of valence. In the PANA model, 
the vertical axis represents low to high positive affect and the horizontal 
axis represents low to high negative affect. The dimensions of valence and 
arousal lay at a 45-degree rotation over these axes (\cite{watson}).

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.4]{images/consensual}
	\centering
	\caption{Consensual (PANA) model of emotion}
	\label{fig:}
\end{figure}

\paragraph{} In 1980, Robert Plutchik constructed a wheel of emotions. This 
model is a hybrid of both basic-complex categories and dimensional theories. 
Emotions are arranged in concentric circles, with the more basic emotions on 
the inner circles, while the outer circles are occupied by complex emotions.  
Notably, outer circles are also formed by blending the inner circle emotions. 
Plutchik suggested 8 primary contrasting pairs of emotions. Joy/sadness, 
anger/fear, trust/disgust and surprise/anticipation. Like colors, primary 
emotions can be expressed at different intensities and can mix with one another 
to form different emotions (\cite{Plutchik1988}).

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/Plutchik_wheel}
	\centering
	\caption{Plutchik's wheel of emotions}
	\label{fig:}
\end{figure}

\paragraph{} The PAD emotional state model was developed by Albert Mehrabian 
and James A. Russel. It uses three numerical dimensions, \textbf{P}leasure, 
\textbf{A}rousal, and \textbf{D}ominance to represent all emotions 
(\cite{mehrabian1980basic}). Initially it's use was in a theory of 
environmental psychology, the core idea being that physical environments 
influence people through their emotional impact (\cite{mehrabian1974approach}). 
Subsequently it was used by Peter Lang to propose a physiological theory of 
emotion (\cite{lang}). Furthermore, it was also used by Russel to develop a 
theory of emotional episodes (\cite{coreaffect}). The Pleasure-Displeasure 
Scale measures how pleasant an emotion may be. Anger and fear are, for 
instance, unpleasant emotions and thus score high on the displeasure. 
Contrarily, joy is a pleasant emotion. The Arousal-Nonarousal Scale measures 
the intensity of emotion. For instance while both anger and rage are unpleasant 
emotions, rage is much more intense than anger. Boredom, while also an 
unpleasant state, has a low arousal volume. Lastly, the 
Dominance-Submissiveness Scale shows the dominant nature of the emotion. While 
both fear and anger are unpleasant emotions, anger is dominant, but fear is a 
submissive emotion (\cite{mehrabian1980basic}).
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.4]{images/pad_model}
	\centering
	\caption{PAD emotional state model}
	\label{fig:}
\end{figure}
A more abbreviated version of the PAD model has also been used in 
organizational studies where the emotions towards specific entities or products 
are measured. It uses just 4 values for each dimension, providing only 64 
values for emotions (\cite{abbrpad}). 

\paragraph{} An example of three-dimensional models, the Lˆvheim cube of 
emotion was presented where the signal substances (dopamine, noradrenaline and 
serotonin) form the axes of a coordinate system, and the eight basic emotions 
according to Tomkins are placed in the eight corners.
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.4]{images/lovheimcube}
	\centering
	\caption{Lˆvheim cube of emotion}
	\label{fig:}
\end{figure}
As shown on the figure below, anger is produced by the combination of low 
serotonin, high dopamine and high noradrenaline. Conversely joy is a product of 
high serotonin, high dopamine and low noradrenaline. Since none of the axis is 
identical to valence \footnote{pleasantness}, the cube seems somewhat rotated 
when 
compared to other models (\cite{Lvheim2012ANT}).


\paragraph{} Most recently Cowen and Kelter, researchers from University of 
California, Berkeley introduced a statistically derived taxonomy of emotion. 
(\cite{Cowen201702247}) \blockquote{Across self-report methods, we find that 
the [2185] 
videos [selected and shown to volunteer subjects] reliably elicit 27 distinct 
varieties of reported emotional experience. Further analyses revealed that 
categorical labels such as amusement better capture reports of subjective 
experience than commonly measured affective dimensions (e.g., valence and 
arousal). Although reported emotional experiences are represented within a 
semantic space best captured by categorical labels, the boundaries between 
categories of emotion are fuzzy rather than discrete. By analyzing the 
distribution of reported emotional states we uncover gradients of emotionófrom 
anxiety to fear to horror to disgust, calmness to aesthetic appreciation to 
awe, and othersóthat correspond to smooth variation in affective dimensions 
such as valence and dominance. Reported emotional states occupy a complex, 
high-dimensional categorical space} 

\paragraph{}More dimensional models of emotion have been developed, though 
there are just a few that remain as the dominant models currently accepted by 
most (\cite{comparison}). There have been observed great cultural differences 
in the way in which emotions are valued, expressed and regulated. The social 
norms for emotions, like the frequency with or circumstances in which they are 
expressed also vary drastically in diverse cultures. An important piece of 
evidence that disputes the universality of emotions is language. Emotions such 
as the schadenfreude \footnote{The experience of pleasure, joy, or 
self-satisfaction that comes from learning of or witnessing the troubles, 
failures, or humiliation of another.} in German and saudade\footnote{Deep 
emotional state of nostalgic or profound melancholic longing for an absent 
something or someone that one loves.} in Portuguese are commonly expressed in 
emotions in their respective languages, but lack an English equivalent. Thus it 
is reasonable in our research to scale back on the complex, culturally 
influenced emotions and focus on the more primal, basic emotions, that may be 
more quantifiable by studying the physiological markers and responses in 
subjects.
\section{Physiological responses of emotions}
\paragraph{}When dealing with physiological effects of emotion, we come across 
a few prevalent theories. The James-Lange theory is one of the earliest 
theories of emotion within modern psychology. It was developed independently 
by 
William James and Carl Lange in 19th century. The basic premise 
\footnote{Which 
is also the point of most criticism} is that physiological arousal instigates 
the experience of emotion, i.e., the arousal precedes and causes the emotion 
(\cite{walter}). According to the Cannon-Bard theory of emotions, the emotion 
is accompanied by physiological arousal.


\paragraph{} In order for us to be able to detect and recognize emotions from 
the data samples, we need to asses which physiological effects of emotions we 
want to capture. Therefore, we need to divide our standard basic emotions 
depending on the combination of kinds and intensity of effects they have on 
human body. There is a difficulty with dealing with physiological markers of 
emotions in that not all emotions do perceivably alter the physiology of the 
subjects. That is the reason, why we need to supplement this data with the 
facial expressions or, as we have tried to confirm, ocular movements. 

\paragraph{} Anger is one of the emotions with more pronounced physiological 
signs. It comes along with faster and deeper breathing 
(\cite{respiratoryfeedback}), increased heart rate, blood pressure, 
perspiration and tensing of muscles. Fear, on the other hand, shares most of 
the surface physiological marks with anger, such as the accelerating breathing 
rate, heart rate and muscle tension, but facial expression and body language 
are vastly different. Even deeper markers, like the cortisol levels differ 
after subjecting a person to these stressors (\cite{Moons2010AngerAF}). Joy, 
sadness, disgust and surprise are similar to each other in their difficulty to 
asses using pure physiological markers and therefore requiring additional 
information, usually in form of facial expression.

\section{Current approaches in detecting and classifying emotions}
\paragraph{}As we've described motion recognition is an important object of 
studies in today's 
psychology, with many potential uses and applications. Correctly assessing and 
recognizing subject's emotion can lead to better understanding it's motivation 
and inner working. Data gained through methods described below can be used to 
assess the effectiveness of marketing, comprehensibility of lectures, usability 
of user interfaces, measure of impact of psychological therapy, etc. Previous 
implementations of emotion recognition technology often have had a multitude of 
disadvantages, which prohibited it's daily and widespread usage. Therefore, we 
have set upon creating a solution, that is modular, reasonable to wear for 
prolonged durations of time and still maintains a degree of reliability in 
captured data. We can do so by using an array of data resources, that 
complement each other, diminishing their disadvantages and reinforcing 
confidence.

\paragraph{}One particularly rich resource for data on human emotions is the 
brain. In 2015 a group of researchers in India published a paper on a system 
using EEG signals as input, Independent Component Analysis \footnote{A 
statistical procedure used for splitting up a set of mixed signals into its 
sources}, Kernel Destiny Estimation \footnote{A method used for feature 
extraction of signals by computing density estimate using kernel-smoothing 
method} and an Artificial Neural Network to  transform the inputs into 
meaningful outputs. They observed better results for clustering of EEG and ECG 
data stream (\cite{LAHANE2015574}). Similar approach was taken by researchers 
in China. Main difference in their approach is that they first applied EMD 
\footnote{Empirical Mode Decomposition} strategy to split EEG signals into a 
series of intrinsic mode functions, which were then fed as sample entropies and 
as feature vectors into SVN\footnote{Support Vector Machine} classifier for 
testing and training. With this approach, they claim to have reached accuracy 
levels of 94.98\% for binary-class tasks and  93.20\% for the multi-class task 
on DEAP database (\cite{zhang}).  Also in 2016, researchers from the Duke 
University, North Carolina demonstrated an emotional recognition technique 
using functional MRI with results, that brain-based models may, in future, 
allow us deeper understanding and assessing emotional status in clinical 
settings, particularly in individuals incapable of providing self-report of 
their own emotional experience (\cite{kragel}). These techniques are, however, 
impossible to replicate on greater scale and in the context of a classroom, or 
other commonplace environment.

\paragraph{}Another indicator of the subject's emotional state is heartbeat. 
Research published in May 2013 in International Journal of Engineering Trends 
and Technology shown compelling data gathering by means of 
ECG\footnote{Electrocardiography} and shown 
differences in ECG signal in subjects in chosen emotional states. 
\footnote{This study was limited to joy, sadness, fear and anger} 
(\cite{shalini2013emotion}). More complex approach was taken by researchers at 
University of Calabria in collaboration with Washington State University. 
Instead of applying a single or a few sensors to study physiological states, 
they used a whole BSN\footnote{Body Sensor Network, specialized Wireless Sensor 
Network applied to the whole human body.} Such networks can include 
accelerometers, gyroscopes, pressure sensors for body movements
and applied forces, skin/chest electrodes (for electrocardiogram
(ECG), electromyogram (EMG), galvanic skin response (GSR), and
electrical impedance plethysmography (EIP)), (PPG) sensors, microphones (for 
voice, ambient, and heart sounds), scalp-placed electrodes for 
electroencephalogram (EEG) (\cite{gravina}). Albeit they were not focusing 
primarily on emotion recognition, this survey shows consequential advancement 
in the state-of-the-art body data collection, especially in the data fusion 
techniques. Similar approach using a variety of sensors and data sources was 
also taken by \cite{MOSCIANO201748} to reasonably accurately classify the two 
dimensions of affect both normal and simulated critical working conditions.

\paragraph{}An interesting approach to emotion detection and classification is 
based upon speech patterns and variations and are better suited for emotions, 
which are otherwise hard to physiologically measure, such as sadness and joy. 
One study shows, that speech signal and feature distances of letters and words 
vary depending on the mood and emotional state of the subject. The study was 
done upon the sample size of 30 people, however, researchers point out, that 
for getting better accuracy, one should consider the data collected from one 
person rather than considering the data from a group of people 
(\cite{DAVLETCHAROVA201591}).

\paragraph{}In terms of evaluating arousal, respiration-based emotion 
recognition shows promise. A study in China showed, that using respiration data 
to evaluate valence and arousal levels of Russel theory, they reached 
classification accuracy of valence and arousal at 73.06\% and 80.78\%, 
respectively (\cite{zhang2}). When compared to other studies using ECG or 
EEG data, the accuracy of valence levels are not exceptional, however the 
classification accuracy of arousal better than most other approaches.

\paragraph{}A study on thermal behavior of anger, disgust, fear, joy and 
sadness was carried out in 2016. When an emotion occurs a change in facial 
temperature appears due to the blood flow that the body emits through blood
vessels in the subcutaneous area (\cite{stephanos}). For example, research 
focused on the emotion of joy, in other words, when a subject is smiling, it has
been found that the temperature of the nose and forehead
decreases during this event (\cite{SALAZARLOPEZ2015149}). Biomedical thermal 
images of the facial
expressions of 44 subjects were captured experiencing the five studied 
emotions, with results of this test at 89.9\% success rate (\cite{Albarran}).

\paragraph{}Another novel approach was experimented with by researchers from 
American University of Sharjah, United Arab Emirates. They have created a 
software touch keyboard, that was installed on Android smartphones, which then 
have been collecting sensor data while users were typing on the keyboard. As 
they have been typing, he or she were prompted to indicate their current 
emotional state, which then tagged the sensor data collected for the particular 
user. Afterwards the data was classified by multiple machine learning 
algorithms to find the best classification method. Based on 
ROC\footnote{Receiver Operating Characteristic} and Precision-Recall curves, it 
was concluded that both J48 and Multi-response linear regression performed
well. This system demonstrates that it is possible to enable emotion 
recognition on mobile phones using built-in sensors (\cite{ZUALKERNAN20171}). 
More research is, nevertheless, needed to precisely ascertain the accuracy and 
applicability of such solution in common practice.

\paragraph{}One approach seems to be very promising, especially due to it's 
implementation in commercial solutions and services. That approach is based on 
facial expression evaluation and it is used by a wide variety of software 
vendors and research institutions. One such research has used Microsoft Kinect 
for 3D face modeling with a goal to computationally recognize facial 
expressions of seven basic emotional states: neutral, joy, surprise, anger, 
sadness, fear and disgust. The subjects of the experiment were six men aged 
26-50 years, told to mimic expressions shown on the screen. Researchers  used 
nearest neighbor classifier (3-NN) and two-layer neural network classifier (MLP)
with 7 neurons in the hidden layer with output accuracy rate of 96\% for 
random division of data and 73\% for "natural" division of data 
(\cite{TARNOWSKI20171175}).
%mention affectiva and other commercial sdk and api

\paragraph{}Last but not least, one of the more recent\footnote{And less 
explored.} approaches taken in affective computing is gaze and pupil tracking. 
This builds on classic theories (\cite{hess} ; \cite{beatty}) and tries to 
implement eye-tracking techniques to investigate emotional state and behavior 
of subjects. They were successful in distinguishing between emotions, albeit 
having to limit them from the core seven to four. Accuracy of their recognition 
ranged in the interval from 80 to 90\% (\cite{maskeliunas})

\paragraph{}There are two questions, that remains unanswered. Is our current 
method of selecting impulses the correct one? Are we rating approaches the 
right way? In research mentioned above, there are mainly two types of 
referential data. One is based on self-identifying emotions, the other on using 
previously assorted categories of pictures or impulses. As we've mentioned 
before, self-identifying emotions is a hard task for most people and it is the 
task, that we aim to solve. Also, is the methodology we use, to personally 
evaluate emotions the correct one? We usually refer to the previous 
evaluations, that trace back to psychologist and we take their emotion 
assessments at face value. It might be wise, that we should ignore previous 
results and rather try to classify and categorize emotions from ground up and 
later compare them with historical data. Finally, most of the researches 
delving into face expression-based emotion recognition declare their statistics 
based upon the success rate of recognizing simple expressions, not emotions 
themselves.

\subsection{Emotion recognition used in commercial environment}
\paragraph{}With plethora of possible applications and use-cases, from 
marketing, automotive industry, health care, education and psychology research, 
it is no wonder that a lot of industries are pushing towards better and faster 
classification of emotions. Consequently the number of dependable 
SDK\footnote{Software development kit -  A set of software development tools 
that allows the creation of application for a certain software package, 
framework, hardware platform, etc.} and API\footnote{Application programming 
interface - A set of functions and procedures that allow the creation of 
applications which access the features or data of an operating system, 
application, or other service.} grows rapidly. A few of them also merge and/or 
work together towards better integration.

\paragraph{}Emotient was one such company. Their FACET SDK allows application  
to track and analyze the emotional responses of users in real-time, detecting 
and tracking expressions of primary emotion, as well as overall positive, 
negative and neutral sentiments and blended composites of two or more emotions. 
In 2013 they partnered with iMotions to provide a facial expression 
recognition, EEG and GSR analysis platform to Procter and Gamble, the United 
States Air Force and Yale University. As part of the agreement between the two 
companies, Emotient's industry-leading emotion recognition solution was fully 
integrated into iMotions Attention Tool, an eye tracking and biosensor software 
platform for research and usability. In 2016 Emotient got acquired by Apple and 
their website is no longer online.

\paragraph{}Next in consideration was Eyeris EmoVu. It's a closed-source, 
multi-platform SDK and API, that supports all three major desktop operating 
systems: Microsoft Windows, macOS and GNU/Linux. It has language hooks for C++, 
Java, VB.NET, Python, Objective-C, Node.js and more. Their pricing starts with 
a free license, that covers the analysis of 500 frames per month. It's declared 
features are \begin{multicols}{2}
	\begin{itemize}
		\item Emotion Recognition
		\item Gender Recognition,
		\item Age Recognition\footnote{Although not in numerical values, but in 
		labels such as Child, Young Adult, Adult and Senior},
		\item Face Recognition,
		\item Facial Tracker,
		\item Engagement and Mood metrics.
	\end{itemize}
\end{multicols}

EmoVu's main drawback is lack of clear information about the SDK availability, 
since they only mention it on their website in unclear terms and the pricing 
only mentions API.

\paragraph{}An interesting approach to confronting racial bias in face 
recognition algorithms is shown by Kairos. As several studies shown, there is a 
great difference between the error rates in human races.  The extent of these 
biases are reflected in an error rate of 0.8 percent for light-skinned men, and 
as high as 34.7\% for dark-skinned women. Since this problem can have very far 
reaching implications, it's important to try to mitigate it. The solution 
advocated by Kairos is to: \begin{enumerate}
	\item Trust the process - as it's a new technology with room to grow,
	\item Improve the data sets - gathering, training, and testing data from a 
	population that is truly representative of global diversity
	\item Seek constant feedback - trying to see what the users and communities 
	see.
\end{enumerate} (\cite{kairos})

\paragraph{}Microsoft, the epitome of a immense corporation, has entered this 
technological race with the launch of their Project Oxford. It's a collection 
of artificial intelligence APIs with focus on computer vision, speech and 
language analysis. It got it's own fair share of controversy with it's mediocre 
launch, but continued to improve steadily. Project Oxford's tools enable face 
detection and emotion detection \footnote{It uses a model of core seven 
emotions plus neutral.} Unfortunately, the API works only with photos. The 
pricing start with a free plan, that covers 30,000 transactions per month.

\paragraph{}InSight SDK is facial recognition C++ toolbox developed by 
Sightcorp in collaboration with the University of Amsterdam. It boasts a large 
set of features, apart from face detection and emotion recognition, it uncovers 
gaze estimation, head pose estimation and motion tracking features. InSight 
does not provide academic trials, therefore we could not evaluate their 
performance.



\paragraph{}Another company that partnered with iMotions is Affectiva. The deal 
came with Affectiva making their Affdex facial coding and emotion analytics 
software available on the iMotions Biometric Research Platform. With this joint 
solution human behavior researchers can combine facial coding and emotion 
analytics with eye tracking, brainwave measurement (EEG), as well as 
physiological sensors (GSR, ECG, EMG) on top of traditional surveys and 
questionnaires, which are also fully integrated. The main Affectiva's 
achievement was the amassment of the world's largest emotion data repository. 
By 2018, they analyzed over 6 million faces from 87 countries The offer both 
API \footnote{Which can be used online on a thin client.} and SDK {Which allows 
applications to run locally and without needing Internet connection.} for the 
major platform like Android, iOS, GNU/Linux, Windows and macOS. They have built 
their massive data set of facial expressions by analyzing millions of face 
videos, of people engaged in various activities, such as watching media 
content\footnote{i.e, ads, movie trailers, television shows and online viral 
campaigns}, driving cars, people in conversational interactions and animated 
gifs\footnote{Via the partnership with giphy}. To get this impressive amount of 
data, Affectiva partnered with their market research partners such as Millward 
Brown, Unruly, Lightspeed, Added Value, Voxpopme and LRW and also with partners 
in the automotive industry, robotics and Human Resources space. Affectiva's 
Affdex SDK was tested on 10 000 images to verify the generalizability of 
algorithms. Those images cover different lightning conditions, both genders, 
various poses of participants, etc. Via the SDK, application can access and 
read multiple metrics. Those include 7 emotion metrics, 20 facial expression 
metrics, 13 emojis and 4 appearance metrics. Apart from the 7 emotional states, 
the SDK surfaces also the Engagement\footnote{A measure of facial muscle 
activation that illustrates the subjectís expressiveness. The range of values 
is from 0 to 100.} and Valence \footnote{A measure of the positive or negative 
nature of the recorded personís experience. The range of values is from -100 to 
100.} metric. The SDK offers four main features. \begin{enumerate}
	\item Face and facial landmark detection,
	\item Face texture feature extraction,
	\item Facial action classification,
	\item Emotion expression modelling.
\end{enumerate} (\cite{affdex}) The SDK are offered under various licensing 
deals, with a free Commercial Evaluation License valid for 60 days, or Student 
Evaluation License valid for 6 months. The API is paid by the processing time, 
with pricing starting at \$1/minute of video processed. Affectiva's mapping of 
expressions onto emotions builds on EMFACS mapping, developed by Friesen and 
Ekman. EMFACS is a variant of FACS system, that considers only emotion-related 
facial actions. (\cite{ekman1997face})
The table \ref{table:1} shows the relationship between the facial expressions 
and the emotions predictors as used in the Affdex SDK.

\begin{table}[h!]
	\centering
	\begin{tabular}{||c | c | c||} 
		\hline
		\hline
		Emotion & Increase Likelihood &  Decrease Likelihood \\ [0.5ex] 
		\hline\hline
		Joy & Smile & Brow Raise, Brow Furrow  \\ 
		\hline
		Anger & Brow furrow, Lid Tighten, & Inner Brow Raise,  \\ 
		& Eye Widen, Chin Raise, & Brow Raise,  \\ 
		&  Mouth Open, Lip Suck & Smile  \\
		\hline
		Disgust & Nose Wrinkle, Upper Lip Raise & Lip Suck, Smile  \\
		\hline
		Surprise & Inner Brow Raise, Brow Raise & Brow Furrow \\
		 & Eye Widen, Jaw Drop & \\
		 \hline
		Fear & Inner Brow Raise, Brow Furrow & Brow Raise, Lip Corner 
		Depressor  \\ 
		& Eye Widen, Lip Stretch & Jaw Drop	Smile \\
		\hline
		Sadness & Inner Brow Raise, Brow Furrow & Brow Raise, Eye Widen, Lip 
		Press  \\ 
		& Lip Corner Depressor & Mouth Open, Lip Suck, Smile \\
		\hline
		Contempt & Brow Furrow, Smirk & Smile  \\ [1ex] 
		\hline
		\hline
	\end{tabular}
	\caption{Relationship between the facial expressions and the emotions 
	predictors}
	\label{table:1}
\end{table}

\begin{table}[h!]
	\centering
	\begin{tabular}{||c | c||} 
		\hline
		\hline
		Increase Positive Valence Likelihood & Increase Negative Valence 
		Likelihood 	\\ [0.5ex] 
		\hline
		\hline
		 Smile & Inner Brow Raise   \\ 
		
		Cheek Raise & Brow Furrow   \\ 
		
		 & Nose Wrinkle  \\
		
		 & Upper Lip Raise  \\
		
		 & Lip Corner Depressor  \\ 
	
		 & Chin Raise   \\ 
		
		 & Lip Press   \\ 
		 & Lip Suck  \\ [1ex] 
		\hline
		\hline
	\end{tabular}
	\caption{Markers increasing and decreasing predicted valence metric. 
	likelihood.}
	\label{table:2}
\end{table}

Lastly, the engagement of the subject is measured as a weighted sum of the 
following facial expressions:
\begin{multicols}{2}
\begin{itemize}
	\item Brow raise,
	\item Brow furrow,
	\item Nose wrinkle,
	\item Lip corner depressor,
	\item Chin raise,
	\item Lip pucker,
	\item Lip press,
	\item Mouth open,
	\item Lip suck,
	\item Smile.
\end{itemize}
\end{multicols}

\paragraph{}Due to the ease of use, clear documentation, offline capabilities 
and free 6 month trial, we have decided to proceed using Affectiva's Affdex SDK.


\chapter{Data gathering methodology}
\paragraph{}As we've demonstrated, there are great many approaches to take and 
physiological markers to explore. Due to the nature of our desired use-case, 
we've decided to omit those physiological signs and pertaining sensors, that 
would excessively constrain and disturb the user. Similarly, we did not employ 
techniques, that would restrict our application from workplace or school 
environment. Therefore, we did not use EEG, fMRI scanning, neither Thermal 
imagining, nor speech recognition. For our body sensors, we have chosen a 
simple, inexpensive optical heart rate sensor, GSR\footnote{Galvanic Skin 
Response} sensor and Arduino to read and resend collected data. Furthermore, we 
used a headset with two high-speed infrared cameras with infrared illumination 
and one 1920x1080 "world" camera. To provide referential emotion values, we 
used a regular 720p web camera, and processed the camera feed with modified 
Affdex sample OpenCV application.
\section{Hardware}
\paragraph{}The research has been done using two different computers. 
\begin{itemize}
	\item Desktop workstation with hexa-core AMD FX-6300 clocked at 4.0GHz, 
	8GB of RAM and Radeon 7750 GPU,
	\item Laptop with quad-core Intel Core i7-4770HQ, 16 GB of RAM and NVidia 
	860M GPU.
\end{itemize}
\paragraph{}These two computers proved able to fully satisfy hardware needs of 
the 
research. We have also tried to run the modifiedq Affdex sample application on 
Raspberry Pi 3B with official IR camera, however we found out, that the single 
board computer was greatly lacking computing power and would freeze, hang and 
drop frames with even one face detected on video feed. Powerful hardware was 
needed not only for the use of Affectiva SDK, but also for the Pupil Capture 
software, supplied by the manufacturer of our headset. These two applications 
taxed both our computers close to peak load, although since our EmoSens data 
collection application requires very few resources, it did not negatively 
impact our experiments.

\paragraph{}The heart rate data was gathered by Pulse Sensor Amped SEN-11574, a 
plug-and-play heart rate sensor for Arduino and Raspberry Pi. It's a simple, 
inexpensive optical sensor, that incorporates amplification and noise 
cancellation circuitry on a single board. We did find out, that it requires 
tight strapping to subject's skin, otherwise the data collected has been 
inconsistent with reality.
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.5]{images/HRCircuit}
	\centering
	\caption{Pulse Sensor Amped SEN-11574 circuitry}
	\label{fig:}
\end{figure}
\newpage

\paragraph{}The Galvanic Skin Response data was collected using Groove GSR 
sensor. Groove is an Arduino shield maker, with a plethora of different sensors 
available to buy. Since we only needed GSR and it is a relatively trivial task 
to wire the Sensor directly to Arduino while circumventing the need for the 
Groove shield itself, we connected the Groove GSR sensor straight to Arduino's 
GPIO pins.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.5]{images/GSRCircuit}
	\centering
	\caption{Groove GSR Sensor circuitry}
	\label{fig:}
\end{figure}


\begin{figure} [ht]
	\centering
	\subfloat[Pulse Sensor Amped SEN-11574]{\includegraphics[width=0.2\linewidth]{images/HRSensor}}%
	\qquad
	\subfloat[Groove GSR 
	Sensor]{\includegraphics[width=0.25\linewidth]{images/GSRSensor}}%
	\caption{Sensors}
	\label{fig:}
\end{figure}

\paragraph{}Both of these sensors were sewn into a fingerless glove and wired 
to Arduino Due connected via USB cable to computer.


\vfill
\begin{figure} [h]
	\centering
	\includegraphics[scale=0.25]{images/arduinoschema}
	\centering
	\caption{ Schema of our sensors wired to Arduino Due}
	\label{fig:}
\end{figure} 
\vfill
\clearpage








\paragraph{}The Pupil headset consists of two high-speed 200Hz IR eye cameras 
with illumination and a single FullHD World camera connected together to a 
computer using USB-A to USB-C connector. All three cameras are attached on a 3D 
printed rigid frame. The IR eye cameras are set on a replaceable joint, 
allowing them to be slightly adjusted to capture eyes of most people. This 
design, however has a slight disadvantage in case the subjects has round head, 
or above average head circumference.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/pupil}
	\centering
	\caption{Pupil headset}
	\label{fig:}
\end{figure} 


\paragraph{}In future iterations of the design, we will swap out Arduino Due 
for an alternative with a smaller footprint. One alternative, that fit's the 
specification would be the BareDuino Nano, with it's 21x14mm size, <2g weight 
\footnote{Compared to the 101.52x53.3mm and 36g Arduino Due} 
and otherwise equivalent with Arduino Uno. This would allow us to sew it inside 
the next version of the glove. If we can add wireless connectivity, such as 
Bluetooth LE, or nRF24L01 module, and Lithium-Ion battery, we can make the 
glove completely cord-free.

\paragraph{}Similarly, with the ever increasing power efficiency and raw 
computing power of small computers such as Intel NUC, or single board computers 
such as ODROID-XU4 or UDOO X86, we could run our experiments




\section{Software}
\paragraph{}Both development and experiments took place on computers running 
Ubuntu 16.04. 
We have chosen this operating system, as it has large community of developers, 
computer scientists and because most innovative frameworks and SDKs target this 
platform. To create the data collection software, we have used the Qt5 
application framework. It's main advantages in our use-case is it's 
intuitiveness, open-source and cross-platform compatibility. It has bindings 
with Python\footnote{PyQt} and C++. It also adds extensions to C++, including 
signals and slots, which greatly simplify handling of events, thus helping in 
development of GUI. It was compiled by GNU GCC 5.4.0. The Arduino is 
programmable in a it's own variant of simplified C++, that forgoes features of 
the language such as inheritance. Last but not least, the modified Affdex 
webcam application is written in C++, albeit due to the incompatibilities with 
newer compilers was compiled in GNU GCC 4.8.

\paragraph{}This leads us to the unexpected setback that we came across. Both 
Affectiva and Pupil use OpenCV 2.4, but Affectiva forked it's version and when 
building with CMake on a recent operating system, it will either fail, or 
overwrite shared libraries, thus breaking QT5 and Pupil. One solution to this 
problem might be to install Affectiva sample application on a separate 
computer, but since we aimed to create a compact solution, we opted for 
installing it in a virtual machine running stock Ubuntu 16.04 with only the 
libraries, that Affectiva requires and nothing else.

\paragraph{}Since Affectiva's licence agreement prohibits it's integration with 
a copyleft open source licensed projects \footnote{In our case GNU GPL v2} we 
have modified the sample OpenCV application to send out data serialized in 
messagepack over zeromq, thus eliminating the need to include it in our 
project. This has added benefit in that our project is therefore modular and 
can use any and all face or other emotion recognition SDK with 
little to no changes to our source code.

\paragraph{}ZeroMQ is a large community of projects with focus on decentralized 
messaging and computing engines. Both software and protocols are open source 
and maintained by community of experts. The original core engine is libzmq, 
available in repositories of most GNU/Linux distributions, but also available 
to install on all the other major operating systems. On top of libzmq, there is 
a large number of language bindings, such as PyZMQ, CZMQ, CPPZMQ, NZMQT 
\footnote{Our project uses NZMQT due to it's great integration with QT5's 
signals and slots}. There are also native languages written for Java, .NET, 
JavaScript and many others. Most importantly, it's really lightweight and fast. 
The core library is only 20K lines of C++ code and the message throughput is 8 
million messages per second with latency in microseconds. There are three main 
patterns used in ZeroMQ. Request-Reply, Publish-Subscribe and Pipeline pattern. 
(\cite{hintjens2013zeromq} ; \cite{Akgul})
 

\paragraph{}MessagePack is a low-overhead binary serialization format. Compared 
to the better know binary serialization format BSON, it's superior in every 
possible technical aspect. It's faster, smaller, and even more compatible to 
JSON than BSON is. It's got support for dozens of languages, notably for 
Python, Java, C, C\# and multiple standards of C++. MessagePack by design 
solves 
the problem of interprocess communication, especially if the processes are 
written in different languages, or are even running on different platforms.

\paragraph{}For our project, we opted to use the Publish-Subscribe pattern, 
with Pupil Capture and Affectiva being the Publishers and our application 
serving as a Subscriber to their messages serialized with MessagePack. That way 
we ensure, that the communication is fast and reliable and also that the 
peculiarities of different programming languages are covered for ease of use. 

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.4]{images/zeromqdia}
	\centering
	\caption{Simplified diagram of the data flow.}
	\label{fig:}
\end{figure}

\paragraph{}For visualization of collected data, we have used two QT libraries. 
The QtCharts library, which is included in the open-source Qt 5.10 
installation, and also the QCustomPlot for more complex and granular control. 
QCustomPlot is a free and open-source Qt C++ widget for plotting and data 
visualization. It has no dependencies and is well documented. It even supports 
exporting to various format such as vectorized PDF files and rasterized 
pictures in PNG, JPG and BMP.

\section{EmoSens application}

\paragraph{}Our main application was written in C++ programming language. The 
GUI was 
created in QtCreator, using the Qt 5.10 framework. After the start of the 
application, the main window is shown. Here, the user can choose whether to 
collect data, or to visualize previously collected data. The QLineEdit on the 
right is auto-filled with the computer's IP address. If user is running Pupil 
Capture and Affectiva application on another computer on the local area 
network, he or she can change the IP address of the server.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/mainwindow}
	\centering
	\caption{Main window of the application}
	\label{fig:}
\end{figure}

\paragraph{}When opened, the Data Collection Dialog polls the selected IP by 
sending a request at port 50200, where Pupil Capture is listening. Pupil then 
replies with the port at which it will publish it's ZeroMQ messages. After 
subscribing to the publisher, EmoSens attempts to connect to the modified 
Affectiva application, using the same IP address and port 5555. Lastly, it will 
open the serial connection on "/dev/ttyACM0" to read data off of Arduino. When 
all of the connections are successful, the dialog will spawn five threads. 
Three threads that receive data from Pupil, one that receives data from 
Affectiva\footnote{Or another face expression application} and one to receive 
data from sensors. Since ZeroMQ can send millions of messages per second, we 
decided to omit most of them and to save only one every 50 milliseconds. The 
dialog runs a timer and every 50 milliseconds will save the current state of 
pupils, heart rate, galvanic skin resistance and facial expressions. All the 
information are simultaneously written to disk with time and date of the 
experiment.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.2]{images/datacollectiondialog}
	\centering
	\caption{Data collection in progress}
	\label{fig:}
\end{figure}

\paragraph{}As shown, there are three buttons on this screen. Start, pause and 
show stats. There are also four real-time charts. the pie chart shows current 
emotion derived from the facial expression. The line graph on the bottom-left 
part shows current heart rate. This does not represent real data, since it's 
extrapolating from 3 second intervals, but is enough to show impulses and 
changes. The bottom-center chart shows galvanic skin response. The last chart 
on the screen is shows subject's emotion valence. This is calculated based upon 
the information on Table 1.2.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.2]{images/livebargraph}
	\centering
	\caption{Current ratio of emotions.}
	\label{fig:}
\end{figure}

\paragraph{}User can, at any time, use the "show stats" button to reveal the 
average ratio of emotions up until the push of the button, represented as a bar 
graph.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.2]{images/livelinegraph}
	\centering
	\caption{Emotions plotted up until this moment}
	\label{fig:}
\end{figure}

\paragraph{}Line chart plotting the emotion variance up until this moment. User 
can scroll horizontally to see the whole detailed chart. The user can also pick 
and choose which metrics or emotions they want to plot on the chart. The user 
can also have multiple charts opened at the same time by pressing the "Show 
stats" button several times.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.2]{images/loaddata}
	\centering
	\caption{QFileDialog - Choose the data file to load.}
	\label{fig:}
\end{figure}

\paragraph{}Going back to the main window, user can instead load a previous 
experiment from the "output" folder. After the file loads\footnote{This can 
take a while, depending on the length of the experiment and power of the 
computer.} you can expose either bar chart showing the ratio of the affects, 
line chart displaying emotions depending on time, or the line chart showing the 
movement of the gaze. This, of course, is the relative gaze of the subjects, 
since the data is collected from the cameras attached to the Pupil headset. 

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.2]{images/loadbargraph}
	\centering
	\caption{Bar chart from the loaded data file.}
	\label{fig:}
\end{figure}

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.2]{images/loadlinegraph}
	\centering
	\caption{Line chart detailing the evolution of emotions during the 
	experiment.}
	\label{fig:}
\end{figure}

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.2]{images/loadgazegraph}
	\centering
	\caption{Line chart detailing the concentration of the subject's gaze.}
	\label{fig:}
\end{figure}

\paragraph{}Note that there is a button allowing user to export his chart to 
vectorized PDF or rasterized image file such as PNG or JPEG.

\section{The experiment}

\paragraph{} For our experiment we have gathered twelve subjects, with equal 
sex ration. The subjects were aged 25-65 years, with various education 
background and upbringing. The experiment we've conducted consisted of having 
the subject watch a 4 minute collection of videos in four categories, depending 
on the emotion, they were supposed to induce. These categories were: 
\begin{itemize}
	\item Joy
	\item Disgust
	\item Fear
	\item Shame/Arousal
	\end{itemize}
Before the experiment, we have explained to the subjects the categories and 
that they should act naturally. We have had trouble getting reliable heart-rate 
data in some subjects, presumably due to the hair on the back of their wrists. 
During the experiment, they were told not to look around, concentrating on the 
screen of the laptop. Whole experience was observed and their behaviour was 
documented with hand-written notes.

\paragraph{} The first part of the shown video was an array of amusing videos. 
It was a collection of various popular "fail" clips on Youtube. The second 
part, disgust, was a long clip of an epidermoid cyst being extracted by a 
dermatologist. Third part was a scary video clip of a ghost jumping in the 
window of a car in a snowstorm. The last segment was an erotic slideshow 
showing attractive naked males, females and both homosexual and heterosexual 
pairs mid coitus. The segments were divided by a few seconds of blank screen to 
allow subjects' emotion to return to neutral. After the experiment, we have 
questioned the subjects about their feelings and emotions. 

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/experiment}
	\centering
	\caption{Setting up the experiment for one of the subjects}
	\label{fig:}
\end{figure}


\chapter{Data evaluation}

\paragraph{}After conducting the experiment, we were left with a lot of data. 
Unfortunately, a few of the subjects did not register with either the hear 
rate, the galvanic skin resistance sensor or neither of the two. This has 
rendered their contribution incomplete. Pupil headset, nevertheless, has been 
very reliable and provided us with full load of data. Full data will be 
provided on the disk with the application, however here are a few interesting 
findings. For the closer look, we have chosen eight subjects.

\section{Subject 1}
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/1_full}
	\centering
	\caption{Subject 1 - Complete emotion graph.}
	\label{fig:}
\end{figure}

\paragraph{}Subject 1 was a tall woman in her thirties. As it is easy to 
discern, 
she did not sit directly and thus her gaze has a slight bias to one corner her 
field of view. 

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/1_gaze}
	\centering
	\caption{Subject 1 - Gaze position.}
	\label{fig:}
\end{figure}

\paragraph{}This is also reinforced by the data from the pupil diameters. Since 
she was looking at the screen slight from the flank, one of her pupil is 
continuously bigger than the other one.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/1_pupil}
	\centering
	\caption{Subject 1 - Pupil diameter.}
	\label{fig:}
\end{figure}

\paragraph{}Subject's biometric data is rather inconclusive, but despite that, 
we can point out that her skin conductivity kept slightly increasing during the 
experiment, having spiked early on, during the "joy-inducing" segment of the 
video. Her heart rate is stable throughout the duration of the video and as per 
her comments, it did not leave a lasting impression.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/1_HRGSRpupil}
	\centering
	\caption{Subject 1 - Biometric information.}
	\label{fig:}
\end{figure}


\paragraph{}Looking at her emotion chart, we can observe slight correlation 
between emotional valence and GSR in the joy-inducing part of the experiment. 
Joy, however, doesn't seem to spike that high, but mixed with slight surprise, 
it drives the valence and engagements metrics up. Affectiva hasn't been able to 
determine disgust, although, by subject's expression, she has been feeling it 
quite strongly. At the end of the experiment, subject got a bit restless, thus 
causing the GSR to behave erratically.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/1_emotions}
	\centering
	\caption{Subject 1 - Sorted emotions.}
	\label{fig:}
\end{figure}
\newpage


\section{Subject 2}
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/2_full}
	\centering
	\caption{Subject 2 - Complete emotion graph.}
	\label{fig:}
\end{figure}
\paragraph{}The second subject was the husband of Subject 1. At similar height, 
we do not see the bias we've seen in the first case, but the head movement did 
influence the gaze information in this case.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/2_gaze}
	\centering
	\caption{Subject 2 - Gaze position.}
	\label{fig:}
\end{figure}

\paragraph{}The different case is here reinforced by the almost equal diameter 
of the pupils. This suggests, that the subject was looking at the screen at 
right angle.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/2_pupil}
	\centering
	\caption{Subject 2 - Pupil diameter.}
	\label{fig:}
\end{figure}


\paragraph{}Here the biometric data suggests, that the subject was the whole 
time rather neutral, only getting aroused in the last segment of the video, 
with both heart rate and skin conductivity consistently growing.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/2_HRGSRpupil}
	\centering
	\caption{Subject 2 - Biometric information.}
	\label{fig:}
\end{figure}

\paragraph{}There is a slight discrepancy between the biometric data and the 
Affectiva's estimation. Although the sensors suggest calm in the first part, 
face expressions were assessed as a sign of engagement. In the end, though, 
both sensors and face expression recognition agreed on a level of arousal.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/2_emotions}
	\centering
	\caption{Subject 2 - Emotions.}
	\label{fig:}
\end{figure}
\newpage
\section{Subject 3}

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.3]{images/3_full}
	\centering
	\caption{Subject 3 - Complete emotion graph.}
	\label{fig:}
\end{figure}

\paragraph{}With 64 year old lady, we can see rather different emotional 
activity. We have a strong GSR and heart rate signal, with slightly indecisive 
pupil and gaze data. 
\newpage
\paragraph{}Having imperfect sights, we can assume she has unconsciously leaned 
towards her better eye, explaining non-existent vertical and large horizontal 
bias.
\newpage
\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.3]{images/3_gaze}
	\centering
	\caption{Subject 3 - Gaze position.}
	\label{fig:}
\end{figure}
\newpage
\paragraph{}Pupil data corroborates our assumption. One pupil was smaller from 
the start of the experiment and in the middle, it dropped altogether. This can 
be explained by leaning head and bigger eyelids covering the field of view of 
the eye camera.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/3_pupil}
	\centering
	\caption{Subject 3 - Pupil diameter.}
	\label{fig:}
\end{figure}


\paragraph{}The raw data indicates turbulent activity at the beginning of the 
test, then tapering off and stabilizing.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/3_HRGSRpupil}
	\centering
	\caption{Subject 3 - Biometric information.}
	\label{fig:}
\end{figure}

\paragraph{}This is confirmed by the last chart, showing great joy and 
engagement in the first segment. Interestingly, we can see contempt spiking 
near the end, when the video converts into the erotic slideshow. We can assume, 
that this change is brought about by different background of the subject.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/3_emotions}
	\centering
	\caption{Subject 3 - Emotions.}
	\label{fig:}
\end{figure}


\section{Subject 4}
\paragraph{}The husband of Subject 3, we can observe vastly different reaction 
to the video shown. Strong heart rate and GSR signal makes this a reliable 
example of the studied phenomena. 
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/4_full}
	\centering
	\caption{Subject 4 - Complete emotion graph.}
	\label{fig:}
\end{figure}
\paragraph{}Gaze seems to be concentrated towards the center, leaning 
downwards. This corresponds with the stature of the subject and the position, 
in which he took part in the experiment.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/4_gaze}
	\centering
	\caption{Subject 4 - Gaze position.}
	\label{fig:}
\end{figure}

\paragraph{}During the experiment, pupils were closely matched, making this a 
high quality sample.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/4_pupil}
	\centering
	\caption{Subject 4 - Pupil diameter.}
	\label{fig:}
\end{figure}


\paragraph{}Subject 4's biometric signal was strong during the whole 
experiment. Especially the relative lack of heart rate signal drops suggest, 
that sensors were attached perfectly. 

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/4_HRGSRpupil}
	\centering
	\caption{Subject 4 - Biometric information.}
	\label{fig:}
\end{figure}

\paragraph{}On subject's emotion chart, we can see engagement in the first 
quadrant of the experiment. This is, however, overshadowed by large activity in 
last quadrant, corroborated by increased biometric signal.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/5_emotions}
	\centering
	\caption{Subject 4 - Emotions.}
	\label{fig:}
\end{figure}


\section{Subject 5}
\paragraph{}This sample is slightly disturbed by the presence of the child of 
the subject. Even then, it can be valuable, as can be seen on the charts below. 
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/5_full}
	\centering
	\caption{Subject 5 - Complete emotion graph.}
	\label{fig:}
\end{figure}
\paragraph{}Due to the presence of the child, the subject's gaze is spread out 
and there is no focal point. This, however, did not disturb the experiment to 
the degree that we anticipated. For this reason, we have kept this sample in 
the evaluation.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/5_gaze}
	\centering
	\caption{Subject 5 - Gaze position.}
	\label{fig:}
\end{figure}

\paragraph{}Due to whole head unconsciously turning towards her child, the 
pupil disparity is relatively low.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/5_pupil}
	\centering
	\caption{Subject 5 - Pupil diameter.}
	\label{fig:}
\end{figure}


\paragraph{}Subject's heart rate and galvanic skin conductivity was very 
erratic during this experiment. Therefore, we had to rely on the facial 
expressions and pupil data to gather information.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/5_HRGSRpupil}
	\centering
	\caption{Subject 5 - Biometric information.}
	\label{fig:}
\end{figure}

\paragraph{}Fitting the pattern of the video, first quadrant is influenced by 
engagement, surprise an joy. Third quadrant sees the increase of pupil 
diameter, valence and engagement. Lastly, the arousing part shows signs of 
surprise and engagement.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/5_emotions}
	\centering
	\caption{Subject 5 - Emotions.}
	\label{fig:}
\end{figure}


\section{Subject 6}
\paragraph{}Subject 6 is a male, 32 year old medical doctor. This has skewed 
the emotional response in an interesting, although predictable way. 
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/7_full}
	\centering
	\caption{Subject 7 - Complete emotion graph.}
	\label{fig:}
\end{figure}
\paragraph{}Subject 6 tends to physically look down, when anticipating 
something fear-inducing. This has obviously influenced the sample of the gaze 
data.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/7_gaze}
	\centering
	\caption{Subject 7 - Gaze position.}
	\label{fig:}
\end{figure}

\paragraph{}The sizes of both pupils are tightly matched. No further influences 
on the sample have been observed.
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/7_pupil}
	\centering
	\caption{Subject 7 - Pupil diameter.}
	\label{fig:}
\end{figure}


\paragraph{}Due to subject's movement during laughter, frightful recoil and 
general restlessness, his biometric signal is rather wandering. However, there 
is still recognizable arousal in the first and the last quadrant of the video.
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/7_HRGSRpupil}
	\centering
	\caption{Subject 7 - Biometric information.}
	\label{fig:}
\end{figure}

\paragraph{}First quadrant of the video has evoked a mixture of contempt, joy 
and surprise, resulting in high levels of engagement. Second segment has 
excited the subject, as his profession has rendered him resistant to disgusting 
views. The last two segments has raised the arousal and excitement levels.
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/7_emotions}
	\centering
	\caption{Subject 7 - Emotions.}
	\label{fig:}
\end{figure}



\section{Subject 7}
\paragraph{}Subject 7 is the wife of the Subject 6. Apart from that, she is a 
medical doctor as well, consequently similar notes concerning previous subject 
apply here too.
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/7_full}
	\centering
	\caption{Subject 7 - Complete emotion graph.}
	\label{fig:}
\end{figure}
\paragraph{}This subject has had no difficulties maintaining concentration and 
focus. She did not look away during the whole experiment.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/7_gaze}
	\centering
	\caption{Subject 7 - Gaze position.}
	\label{fig:}
\end{figure}

\paragraph{}Subject's pupil sizes, albeit not identical, follow the same trend, 
signifying correct position of the headset. 
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/7_pupil}
	\centering
	\caption{Subject 7 - Pupil diameter.}
	\label{fig:}
\end{figure}


\paragraph{}Not considering few random peaks in the galvanic skin conductivity, 
subject's physiological signs were level the whole experiment, with only the 
heart rate oscillating in the last segment.
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/7_HRGSRpupil}
	\centering
	\caption{Subject 7 - Biometric information.}
	\label{fig:}
\end{figure}

\paragraph{}Interestingly, this person has experienced peaks of disgust, 
engagement and joy in each section of the video. Pupil size, nor other 
physiological markers do not seem to correlate in this example.
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/7_emotions}
	\centering
	\caption{Subject 7 - Emotions.}
	\label{fig:}
\end{figure}


\section{Subject 7}
\paragraph{}Subject 7 is the wife of the Subject 6. Apart from that, she is a 
medical doctor as well, consequently similar notes concerning previous subject 
apply here too.
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/7_full}
	\centering
	\caption{Subject 7 - Complete emotion graph.}
	\label{fig:}
\end{figure}
\paragraph{}This subject has had no difficulties maintaining concentration and 
focus. She did not look away during the whole experiment.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/7_gaze}
	\centering
	\caption{Subject 7 - Gaze position.}
	\label{fig:}
\end{figure}

\paragraph{}Subject's pupil sizes, albeit not identical, follow the same trend, 
signifying correct position of the headset. 
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/7_pupil}
	\centering
	\caption{Subject 7 - Pupil diameter.}
	\label{fig:}
\end{figure}


\paragraph{}Not considering few random peaks in the galvanic skin conductivity, 
subject's physiological signs were level the whole experiment, with only the 
heart rate oscillating in the last segment.
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/7_HRGSRpupil}
	\centering
	\caption{Subject 7 - Biometric information.}
	\label{fig:}
\end{figure}

\paragraph{}Interestingly, this person has experienced peaks of disgust, 
engagement and joy in each section of the video. Pupil size, nor other 
physiological markers do not seem to correlate in this example.
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/7_emotions}
	\centering
	\caption{Subject 7 - Emotions.}
	\label{fig:}
\end{figure}





\section{Subject 8}
\paragraph{}Subject 8 is also a medical doctor. Her data was different, though, in that it spiked with engagement at every start of a segment, but would taper of afterwards.
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/8_full}
	\centering
	\caption{Subject 8 - Complete emotion graph.}
	\label{fig:}
\end{figure}
\paragraph{}This subject has had no difficulties maintaining concentration and 
focus. She did not look away during the whole experiment.

\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/8_gaze}
	\centering
	\caption{Subject 8 - Gaze position.}
	\label{fig:}
\end{figure}

\paragraph{}Subject's pupil sizes, albeit not identical, follow the same trend, 
signifying correct position of the headset. 
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/8_pupil}
	\centering
	\caption{Subject 8 - Pupil diameter.}
	\label{fig:}
\end{figure}


\paragraph{}Not considering few random peaks in the galvanic skin conductivity, 
subject's physiological signs were level the whole experiment, with only the 
heart rate oscillating in the last segment.
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/8_HRGSRpupil}
	\centering
	\caption{Subject 8 - Biometric information.}
	\label{fig:}
\end{figure}

\paragraph{}Interestingly, this person has experienced peaks of disgust, 
engagement and joy in each section of the video. Pupil size, nor other 
physiological markers do not seem to correlate in this example.
\begin{figure} [ht]
	\centering
	\includegraphics[scale=0.3]{images/8_emotions}
	\centering
	\caption{Subject 8 - Emotions.}
	\label{fig:}
\end{figure}





\chapter{Discussion}
\paragraph{}This diploma thesis dealt with methods of extraction and classification of human emotions. We have shown, that there is a possibility of a correlation between the change in pupil size, gaze movement and emotion. We have consolidated data from multiple sources, such as sensors capturing physiological changes, camera stream processed by machine learning algorithms and recognizing facial expressions and last but not least, high-speed gaze-tracking camera system. However, we suggest, that there is a reasonable doubt considering the methodology of assessing success rate in affective computing. During our experiments, we have witnessed many people of various background and age not emote at all, while watching our testing clip and also people, that overcompensate for their lack of emotion with exaggerated facial expressions. It is hard, even for the most emphatic people to guess other peoples emotions. We argue, that many affective computing success stories are little more than an exercise in face recognition and there is still a long way to a true emotion recognition. 

\paragraph{}What needs to change in order for us to reach real affective computing?
\begin{itemize}
	\item We need to doubt our current methodologies until we either prove them effective or replace them with new.
	\item We need to determine, how to evoke real emotions,
	\item We need to create a way to quantify real emotions.
\end{itemize}

\paragraph{}We, as computer scientists, should not put blind trust into old models and theories, that came around in ancient times. These models are more often than not built upon unstable assumptions. We need to disregard old philosophies and should rather build on the recent knowledge. We should work together with neuroscientists an psychiatrists to capture the mechanism of human emotion.

\paragraph{}In order to determine how human emotions actually work, we need to be able to evoke them in a laboratory conditions. However, our current methods of evoking emotions are not enough. We suggest utilizing recent developments in VR technologies, to create more captivating experiences. Ones that will enable the subject's forget their current situation and live in the manufactured moment. This could be done by combining 
%amnesia inducing drugs,
multiple human senses. Haptic feedback, taste and smell synthesizers, high definition surround video and audio.
%citation needed.

\chapter*{Zhrnutie}
\paragraph{}EmÛcie patria k najosobnejöÌm Ëastiam æudskej existencie, hoci s˙ veæmi ùaûko definovateæne, rozpoznateænÈ a pomenovateænÈ, i v n·s samotn˝ch. Uû zo svojej podstaty sa lÌöia od Ëloveka k Ëloveku, v rÙznych kult˙rach a v˝chov·ch. Naöe ponÌmanie sa vöak neust·le vyv˝ja a zdokonaluje. RÙzni autori sa snaûili rozdeliù naöe citovÈ stavy na z·kladnÈ kategÛrie, naprÌklad Paul Ekman na Hnev, Strach, Znechutenie, äùastie, Sm˙tok a Prekvapenie. Naproti tomu s˙ËasnÌ psycholÛgovia i historici ukazuj˙, ûe je na mieste komplexnejöÌ pohæad na emÛcie.

\paragraph{}Rozpozn·vanie emÛcii sa v dejin·ch neust·le menÌ. V 12. storoËÌ sa bardi nepozerali na zÌvanie ako na prejav nudy, Ëi ˙navy, ale ako na prejav skrytej a hlbokej l·sky. SkorÌ kresùania pozn·vali emÛciu, ktor˙ naz˝vali "accidie", formu letargie a z˙falstva prin·öan˙ lietaj˙cimi dÈmonmi. Znudenie je emÛcia, ktor˙ prv˝kr·t pociùovali a pomenovali Viktori·nci, ako prirodzen˙ reakciu na novÈ idei voænÈho Ëasu a sebazdokonalovania. Medzi psycholÛgmi dodnes nejestvuje konsenzus, Ëi rÙzne kult˙ry pociùuj˙ niektorÈ emÛcie silnejöie, keÔûe sa un˙vali ich pomenovaù vlastn˝mi slovami. Rusk· "toska", t˙ûba s niËÌm, po Ëom by Ëlovek mohol t˙ûiù, ako ju definoval Nabokov. Do 17. storoËia sa datuje prÌbeh o ötudentovi, ktorÌ ûil v ävajËiarskom univerzitnom meste Basel. Ochorel, mal hor˙Ëky, srdeËnÈ palpit·cie, vyr·ûky na pokoûke a bol blÌzko smrti. KeÔ ho lek·ri poslali domov, aby umrel pri rodine, zaËalo sa mu uæavovaù. KeÔ zastal pred br·nami svojho rodnÈho mesta, uû bol skoro ˙plne vylieËen˝. V 1688 lek·r Johannes Hoffer ötudoval tento a inÈ podobnÈ prÌpady a nazval t˙to formu v·ûnej clivosti za domovom nostalgiou. Posledn· zaznamenan· smrù nostalgiou sa stala americkÈmu vojakovi poËas Prvej Svetovej Vojny vo Franc˙zsku. ZaËiatkom 20. storoËia sa tento pocit zmenil skÙr na t˙ûbu po minulosti a stratil na v·ûnosti.

\paragraph{}V tejto diplomovej pr·ci pojedn·vame viacero s·d citov˝ch znakov a technÌk ako ich rozpoznaù. Tieto metÛdy sa najËastejöie snaûia detegovaù z·kladnÈ emÛcie, ako naprÌklad öesù z·kladn˝ch emÛcii Paula Ekmana, alebo 12 bodov˝ kruhov˝ model afektov.

\paragraph{}Empatick· v˝poËetn· technika m· veækÈ moûnosti vyuûitia. Rozpozn·vanie emocion·lnych stavov mÙûe zlepöiù v˝chovnovzdel·vacÌ proces, poskytovaù cenn˙ sp‰tn˙ v‰zby pre uËiteæov, psychiatrov. MÙûe pomÙcù pri zvl·danÌ autizmu. Okrem toho n·jde uplatnenie i v mnoh˝ch oblastiach priemyslu. Automobilky, cielen˝ marketing, robotickÈ zvierat· Ëi zdravotnÈ sestry.

\paragraph{}Problematika klasifik·cie emÛcii je predmetom ûivej debaty. Dva z·kladnÈ postoje emoËn˝ch vedcov s˙:
\begin{enumerate}
	\item EmÛcie s˙ diskrÈtne a fundament·lne odliönÈ konötrukty,
	\item EmÛcie sa prelÌnaj˙ a s˙ charakterizovanÈ na dimenzion·lnej b·ze v zoskupeniach.
\end{enumerate}

\paragraph{}DiskrÈtne modely emÛcii stavaj˙ na tÈze, ûe existuje malÈ mnoûstvo z·kladn˝ch afektov Toto ËÌslo sa lÌöi od proponenta k proponentovi, naprÌklad Silvan Tomkins rozpozn·val dev‰ù z·kladn˝ch emÛciÌ. To s˙ z·ujem-nadöenie, poteöenie-radosù, prekvapenie-zæaknutie, nepokoj-˙zkosù, hnev-z˙rivosù, strach-des, hanbu-ponÌûenie a znechutenie-znecÌtenie. Z p·rov je vûdy prv˝ miernym prejavom emÛcie a druh˝ extrÈmnym   Carroll Izard oproti tomu definoval dvan·sù diskrÈtnych emÛciÌ: z·ujem, radosù, prekvapenie, sm˙tok, hnev, znechutenie, poh‡danie, sebe-nepriateæstvo, strach, ponÌûenie, hanblivosù a pocit viny. 

\paragraph{} Paul Ekman a Carroll Izard uskutoËnili niekoæko medzikult˙rn˝ch öt˙diÌ a zreferovali objav aspoÚ öiestich emÛcii, ktorÈ æudia po celom svete prejavuj˙ a rozpozn·vaj˙. Toto dok·zali, keÔ navötÌvili Nov˙ Guineu a izolovan˝m domorodcom dali rozpoznaù sadu emÛcii z fotiek aktÈrov.

\paragraph{}Jestvuje viacero praktick˝ch a teoretick˝ch dÙvodov preËo niektorÌ definuj˙ emÛcie pomocou jednej Ëi viacer˝ch dimenzÌ. V‰Ëöina z modelov zakomponuj˙ valenciu a vzruöenie, respektÌve intenzitu ako dimenzie. Wilhelm Max Wundt navrhol, ûe emÛcie mÙûu byù definovanÈ v troch rozmeroch - prÌjemne-neprÌjemne, vzruöuj˙ce-tlmiace, ˙navne-relaxaËne. Harold Schlosberg pomenoval tri dimenzie: prijemne-neprijemne, pozornost-odmietnutie a stupeÚ aktiv·cie. In˝ model bol vyvinut˝ Jamesom Russellom. Tento model emÛcie rozmiestÚuje na dvojrozmernom kruhovom priestore, obsahuj˙com dimenziu vzruöenia a valencie. œaæöÌm modelom je vektorov˝, ktor˝ pozost·va z vektorov vûdy mieriacich dvoma smermi.

\paragraph{}V 1980 Robert Pluchik skonötruoval koleso emÛcii. Tento model je hybridom medzi z·kladn˝mi kategÛriami a dimenzion·lnymi teÛriami. EmÛcie s˙ usporiadanÈ v koncentrick˝ch kruhoch, s z·kladn˝mi emÛciami dnu a komplexn˝mi emÛciami po vonkajöej strane.

\paragraph{}PAD model \footnote{Pleasure, Arousal, Dominance} pouûÌva tri ËÌselnÈ rozmery na definovanie vöetk˝ch emÛciÌ. PrÌkladom trojrozmern˝ch modelo je Lovheimova kocka emÛciÌ, kde osy kocky tvoria sign·lne substancie a vrcholy z·kladnÈ emÛcie. NaprÌklad hnev je podæa tohto modelu tvoren˝ nÌzkym serotonÌnom, vysok˝m dopamÌnom a noradrenalÌnom.


\paragraph{}KeÔ sk˙mame fyziologickÈ symptÛmy emÛciÌ, natrafÌme na dve z·kladnÈ teÛrie. TeÛria James-Lange tvrdÌ, ûe telesnÈ vzruchy predch·dzaju a spÙsobuj˙ emÛcie. Podæa teÛrie Cannon-Bard emÛcia je sprev·dzan· fyziologick˝m vzruchom.

\paragraph{}Aby sme mohli detegovaù a rozpozn·vaù emÛcie zo vzoriek, potrebujeme zhodnotiù, ktorÈ fyziologickÈ prejavy emÛciÌ chceme zaznamen·vaù. Hnev je jedna z emÛciÌ s v˝raznejöÌmi fyziologick˝mi zn·mkami. Sprev·dza ho r˝chlejöie d˝chanie, srdcov˝ tep, vyööÌ krvn˝ tlak, napÌnanie svalov. Na uæahËenie detekcie si pom·hame dodatoËn˝mi d·tami zÌskan˝mi z v˝razov tv·re.

\paragraph{}Pre potreby tejto pr·ce sme si vybrali nasledovnÈ zdroje d·t: Senzor srdeËnÈho tepu, senzor galvanickej odozvy koûe, headset Pupil s dvoma vysokor˝chlostn˝mi infraËerven˝mi kamerami snÌmaj˙cimi oËi a jednou s vysok˝m rozlÌöenÌm snÌmaj˙cou okolie. Senzory s˙ pripojenÈ na Arduino DUE, ktorÈ impulzy spracov·va a posiela po sÈriovom pripojenÌ do poËÌtaËa. Na zÌskavanie referenËn˝ch hodnÙt emÛciÌ pouûÌvame video z webkamery v re·lnom Ëase spracov·vanÈ softvÈrom na rozpozn·vanie emÛciÌ z v˝razov tv·re. V tomto prÌpade sa jedn· o program vyuûÌvaj˙ci SDK komerËnÈho rieöenia Affekctiva. VÔaka modul·rnosti n·öho dizajnu programu vöak mÙûeme po uplynutÌ öesùmesaËnej ötudentskej licenËnej doby vymeniù t˙to Ëasù za æubovoæn˝ softvÈr na spracovanie tv·rÌ.
\paragraph{} Na komunik·ciu medzi procesmi pouûÌvame kniûnicu ZeroMQ, ktor· m· implement·cie v mnoh˝ch jazykoch a na vöetk˝ch majoritn˝ch platform·ch. Spr·vy posielanÈ cez ZeroMQ s˙ balenÈ do bajtovÈho form·tu MessagePack. T˝m zamedzÌme nekompatibilite medzi rÙznymi jazykmi a platformami.

\paragraph{}Naöa aplik·cia EmoSens je napÌsan· v jazyku C++ s frameworkom QT vo verziÌ 5.10. Od tejto verzie obsahuje open source verzia QT novÈ moduly QtCharts a QtDataVisualization. Okrem toho sme do aplik·cie integrovali aj kniûnicu tretej strany QCustomPlot, ktor· prin·öa dodatoËnÈ funkcie na vykreslovanie pokroËil˝ch grafov. Aplik·cia pozost·va z troch okien. HlavnÈ okno sl˙ûi na definovanie IP adresy servra, na ktorom je spusten· Affectiva a Pupil Capture. Z hlavnÈho okna sa d· spustiù okno zbierania ˙dajov. Toto okno najskÙr vyöle dotaz na port 50200, na ktor˝ Pupil odpovie ËÌslom portu vydavateæa spr·v. N·sledne sa EmoSens pripoj˙ na port vydavateæa, port affectivy a na sÈriovÈ pripojenie Arduina. Potom spustÌ vöetky pripojenia a zaËne kaûd˝ch 50ms ukladaù aktu·lne d·ta do s˙boru s Ëasovou znaËkou experimentu a vykreslovaù ich do grafu na obrazovke. N·sledne m· pouûÌvateæ moûnosù kliknutia na zobrazenie ötatistiky. Tam m· moûnosù zobrazenia stÂpcovÈho grafu priemern˝ch emÛciÌ za dobu experimentu, prÌpadne ËiarovÈho grafu vykresluj˙ceho emÛcie a metriky poËas celej doby experimentu. Po ukonËenÌ experimentu mÙûe pouûÌvateæ z hlavnej obrazovky programu vybraù vizualiz·ciu predch·dzaj˙cich experimentov. Tu m· detailnejöie zobrazenÈ emoËnÈ stavy aj s mapou pohybu pohæadu testovanÈho subjektu. 


\paragraph{}Testovali sme dokopy 12 subjektov vo veku 25-65 rokov s rÙznym vzdelanÌm a z·zemÌm. Nakoniec sme z toho vybrali 8 reprezentatÌvnych subjektov, ktorÈ tu bliûöie popÌöeme. Testovali sme ich tak, ûe sme im pustili video navodzuj˙ce ötyri emoËnÈ stavy - radosù-pobavenie, znechutenie, strach a zahanbenie-vzruöenie.

\paragraph{}Subjekt 1 bola vysok· ûena okolo 30 rokov. Podæa grafu je jednoznaËne viditeænÈ, ûe nesedela rovno a jej pohæad mal sklz do rohu zornÈho poæa. Toto potvrdili aj ˙daje o veækosti öoöoviek. BiometrickÈ ˙daje boli znaËne nejednoznaËnÈ, ale napriek tomu mÙûeme podotkn˙ù, ûe jej vodivosù pokoûky mierne st˙pala poËas pokusu. Jej srdeËn˝ rytmus bol stabiln˝. Na emoËnom grafe mÙûeme pozorovaù mal˙ korel·ciu medzi valenciou a GSR poËas prvÈho segmentu eperimentu.
\paragraph{}Subjekt 2 bol manûel prvÈho subjektu. Subjekt 2 mal poËas experimentu skoro totoûnÈ priemery oboch öoöoviek. Biometria ukazuje, ûe bol poËas celej doby v neutr·lnom stave, avöak ku koncu nar·stla miera vzruchu.

\paragraph{}TretÌ subjekt bola 64 roËn· ûena. U nej pozorujeme siln˝ sign·l GSR a srdeËnÈho tepu s mierne nerozhodn˝mi d·tami oËÌ. D·ta ukazuj˙ na prudk˙ aktivitu na zaËiatku testu, neskÙr sa hladina vyrovnala a stabilizovala.

\paragraph{} Subjekt 4 je manûel subjektu 3. Taktieû je tu badateæn˝ siln˝ sign·l zo senzorov. S˙stredn˝ bod pohæadu bol na spodu a v strede zornÈho poæa. Zaznamen·vame vzruchy na zaËiatku i konci experimentu

\paragraph{} Subjekt 5 m· d·ta naruöenÈ prÌtomnosùou jej dieùaùa. Vidno to z toho, ako jej pohæad sa neust·lil na ûiadnej oblasti, zato je rozpt˝len˝. Prv· Ëasù videa vyvolala prekvapenie a pobavenie. Tretia Ëasù, valenciu a zaujatia. Nakoniec posledn· Ëasù znovu zaznamenala prekvapenie a zaujatie.

\paragraph{} Subjekt 6 je 32 roËn˝ muû, lek·r. Aj toto pravdepodobne ovplyvnilo jeho v˝sledky, v ktor˝ch prakticky nefiguruje znechutenie. Jeho ostatnÈ d·ta vöak presne koreöponduj˙ s pohybmi, ktorÈ poËas testu mimovoæne robil.

\paragraph{} Subjekt 7 je manûelka Subjektu 6, taktieû lek·rka. Z toho dÙvodu s˙ jej emoËnÈ hodnoty podobnÈ ako v predch·dzaj˙com prÌpade. LÌöia sa vöak mierou s˙stredenia pohæadu, ktor˝ je v tomto prÌpade omnoho vyööÌ.

\paragraph{}Subjekt 8 je taktieû lek·rka, avöak vo veku 62 rokov. U nej boli pozorovateænÈ kr·tkodobÈ stropy emÛciÌ, vûdy pri zaËiatku segmentu, avöak potom upadali.

\paragraph{} T·to diplomov· pr·ca sa zaoberala metÛdami extrakcie a klasifik·cie æudsk˝ch emÛciÌ. Preuk·zali sme, ûe existuje moûn· korel·cia medzi zmenami veækosti zreniËky, pohybov oËÌ a emÛciami. Konsolidovali sme d·ta z viacer˝ch zdrojov, naprÌklad zo senzorov meraj˙cich fyziologickÈ zmeny, kamerovÈho z·znamu v˝razov tv·re Ëi vysokor˝chlostn˝ch kamier snÌmaj˙cich oËi. Napriek tomu chceme zmieniù, ûe existuje rozumn· pochybnosù ohæadom metodolÛgie hodnotenia ˙speönosti v rozpozn·vanÌ emÛciÌ. Pozorovali sme subjekty, ktorÈ navonok emÛcie vÙbec neprejavovali, hoci ich v rozhovore po z·vere experimentu popisovali. Taktieû sme videli æudÌ, ktorÌ v prostredÌ experimentu mimovoæne preafektov·vali svoje reakcie a t˝m ovplyvÚovali v˝sledky. Je pravdepodobnÈ, ûe mnoûstvo popul·rnych softvÈrov nie je skutoËn˝m rozpozn·vanÌm emÛciÌ, ale len vzorov pohybov tv·re, bez nosnej emÛcie.

\paragraph{}»o by sa malo zmeniù, aby sme dosiahli skutoËnÈ strojovÈ rozpozn·vanie emÛciÌ? Mali by sme zruöiù alebo upraviù st·vaj˙ce metodolÛgie. Mali by sme sk˙maù, ako evokovaù re·lne emÛcie a mali by sme n·jsù spÙsob ich kvantifikovaù.

\paragraph{}Na tieto ˙Ëely by sme do bud˙cna mali vyuûiù r˝chly v˝voj v obore virtu·lnej reality, ktor· mÙûe Ëloveku prezentovaù omnoho realistickejöie z·ûitky, ako video alebo fotky na obrazovke. Obzvl·öù, ak ju obohatÌme o syntetiz·tori vÙnÌ a chutÌ, haptickou odozvou a vyööou kvalitou zvuku a videa. 
\printbibliography
\end{document}